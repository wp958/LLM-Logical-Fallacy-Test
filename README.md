# LLM-Logic-Eval-Framework
An automated evaluation framework for LLM logic reasoning, testing 8 types of logical fallacies using Python. 
# LLM Logic Evaluation Framework (å¤§æ¨¡å‹é€»è¾‘æ¨ç†è¯„æµ‹æ¡†æ¶)

## ğŸ“– Project Introduction (é¡¹ç›®ç®€ä»‹)
This project is an automated framework designed to evaluate the logical reasoning capabilities of Large Language Models (LLMs).
(æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªé’ˆå¯¹å¤§è¯­è¨€æ¨¡å‹é€»è¾‘æ¨ç†èƒ½åŠ›çš„è‡ªåŠ¨åŒ–è¯„æµ‹æ¡†æ¶ã€‚)

It focuses on resisting logical fallacies, including **Curry's Paradox (åº“é‡Œæ‚–è®º)**, **Gambler's Fallacy (èµŒå¾’è°¬è¯¯)**, and **Quantifier Ambiguity (é‡è¯æ­§ä¹‰)**.

## ğŸš€ Key Features (æ ¸å¿ƒåŠŸèƒ½)
- **Logic Probes:** 8 types of logical traps designed based on Formal Logic.
- **Auto-Evaluation:** Python scripts utilizing Spark API for batch testing.
- **Metric Visualization:** Confusion matrix analysis and confidence score distribution.

## ğŸ› ï¸ Tech Stack (æŠ€æœ¯æ ˆ)
- **Language:** Python 3.9
- **Libraries:** Pandas, NetworkX, Matplotlib, Jieba, Requests
- **Model:** Spark X1 (via API)

## ğŸ“Š Results (ç»“æœå±•ç¤º)
> Here you can upload your charts!
<img width="3000" height="1800" alt="accuracy_by_module" src="https://github.com/user-attachments/assets/aaae655b-4adf-4a05-857b-cbac183f1817" />
<img width="3600" height="2400" alt="fallacy_confusion_matrix" src="https://github.com/user-attachments/assets/8cdc8d92-e866-44c7-8dd5-605eaddee81f" />
<img width="3000" height="1800" alt="confidence_distribution" src="https://github.com/user-attachments/assets/bf6261c5-b357-4b14-89ec-0a85fc50baae" />
The evaluation shows that while LLMs perform well in formal logic, they suffer from the **Dunning-Kruger Effect** in semantic ambiguity tasks.
## ğŸ‘¨â€ğŸ’» Author
**Wang Pan**
Philosophy Student @ Xiangtan University
Focusing on AI & Logic.
